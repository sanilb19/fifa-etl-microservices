# FIFA ETL Pipeline with Dask & PostgreSQL

This project demonstrates a scalable, containerized ETL pipeline using:
- **Dask** for parallel data processing
- **PostgreSQL** as the backend database
- **Docker Compose** for service orchestration

We use historical FIFA player data (2015‚Äì2021) and process it through an ETL pipeline, now implemented as independent microservices.

---

## üìÅ Project Structure

```
.
‚îú‚îÄ‚îÄ config/                   # Configuration and settings
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ archive/              # (Optional) Archive of raw CSVs
‚îÇ   ‚îú‚îÄ‚îÄ kagglehub/            # Downloaded Kaggle datasets
‚îÇ   ‚îú‚îÄ‚îÄ processed/            # Transformed data (Parquet)
‚îÇ   ‚îî‚îÄ‚îÄ raw/                  # Raw CSVs for processing
‚îú‚îÄ‚îÄ docker/                   # Dockerfiles and related scripts
‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îú‚îÄ‚îÄ extract/              # Extraction microservice
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ extract.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ main.py
‚îÇ   ‚îú‚îÄ‚îÄ transform/            # Transformation microservice
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ transform.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ main.py
‚îÇ   ‚îú‚îÄ‚îÄ load/                 # Loading microservice
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ load.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ main.py
‚îÇ   ‚îî‚îÄ‚îÄ api/                  # (Optional) API service
‚îú‚îÄ‚îÄ tests/                    # Test suite
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ requirements-base.txt
‚îú‚îÄ‚îÄ requirements-optional.txt
‚îú‚îÄ‚îÄ requirements-dev.txt
‚îú‚îÄ‚îÄ docker-compose.yml
‚îî‚îÄ‚îÄ README.md
```

---

## üöÄ Getting Started

### 1. Clone the Repo

```sh
git clone https://github.com/yourname/fifa-etl-dask.git
cd fifa-etl-dask
```

### 2. Set Up Environment Variables

Create a `.env` file in the project root (see `config/settings.py` for defaults):

```
DB_CONNECTION_STRING=postgresql://postgres:postgres@postgres:5432/fifa_db
KAGGLE_DATASET=stefanoleone992/fifa-21-complete-player-dataset
DASK_WORKERS=4
DASK_MEMORY_LIMIT=4GB
```

> **Note:** For Kaggle downloads, place your `kaggle.json` credentials in `~/.kaggle/`.

### 3. Build and Run the Pipeline

```sh
docker-compose up --build
```

This will:
- Start a PostgreSQL database (`postgres`)
- Run the ETL pipeline as three sequential services:
  - `extract`: Downloads and prepares raw data
  - `transform`: Cleans and transforms data, outputs Parquet
  - `load`: Loads transformed data into PostgreSQL

Data is shared between services via the `data/` directory.

---

## üß™ Verifying the Pipeline

- Check logs for each service:
  ```sh
  docker-compose logs extract
  docker-compose logs transform
  docker-compose logs load
  ```
- Connect to the database:
  ```sh
  docker exec -it <container_id_of_postgres> psql -U postgres -d fifa_db
  ```
  Then try:
  ```
  \dt
  SELECT * FROM fifa_players LIMIT 10;
  ```

---

## üßº Clean Up

```sh
docker-compose down -v  # Stops and removes containers and volumes
```

---

## üß† Extending This Project

- Add more ETL transformations (cleaning, feature engineering)
- Create additional microservices for new steps
- Add a FastAPI/Flask service to query enriched data
- Integrate with streaming platforms (Kafka, RabbitMQ)
- Deploy to the cloud (ECS, GKE, Lambda, etc.)

---

## ü§ù Why This Project?

- Demonstrates Python, Docker, and Dask proficiency
- Shows experience with containerized microservices
- Covers database, ETL, and scalable workflow best practices

